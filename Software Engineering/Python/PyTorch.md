
PyTorch is the premiere [Deep Learning](../../Machine%20Learning/Deep%20Learning/Neural%20Networks.md) framework in [Python](Python%20101.md).

## Tensor

The fundamental matrix library for PyTorch is *Tensor*, which has an almost identical API to NumPy.

## Dataset & DataLoader

PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and the `DataLoader` wraps an iterable around `Dataset` to enable easy access to the samples.

When you want to create a custom dataset, there are three functions that must be implemented. `__init__` is run once to instantiate the dataset object. We initialize the locations of the data, the labels, and the transforms if specified. The `__len__` simply returns the number of samples in our dataset. The `__getitem__` function loads and returns a sample from the dataset at the given `index`.

```python
from torch.utils.data import Dataset

class CustomImageDataset(Dataset):
	def __init__(self, data_dir, labels, transform=None, target_transform=None):
		self.data_dir = data_dir
		self.labels = labels
		self.transform = transform
		self.target_transform = target_transform

	def __len__(self):
		return len(self.labels)

	def __getitem__(self, index):
		# logic to fetch a particular piece of data
		if self.transform:
			data = self.transform(data)
		if self.target_transform:
			label = self.target_transform(label)
		return data, label
```

PyTorch's `DataLoader` is responsible for managing batches. You can create a `DataLoader` from any `Dataset`. The `DataLoader` is an iterable that we can use to abstract the functions of iterating through the dataset in parallel and in batches. You simply pass in the dataset, the [batch size](../../Machine%20Learning/Deep%20Learning/Optimizers.md) and any other parameters like shuffling after every epoch.

## Torch.nn

While we have the ability to manually set the forward and backprop step of every layer of a model, PyTorch provides a higher level API in the form of `torch.nn`. This method allows you to define roughly the "layers" of your [Neural Networks](../../Machine%20Learning/Deep%20Learning/Neural%20Networks.md). You can also define whatever [Activation Functions](../../Machine%20Learning/Deep%20Learning/Activation%20Functions.md) you want, as well as any other special type of layer like [Convolutional layers](../../Machine%20Learning/Deep%20Learning/Convolutional%20Neural%20Net.md). Here is a 3 layer ConvNet:

```python
model = nn.Sequential(
    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),
    nn.ReLU(),
    nn.AdaptiveAvgPool2d(1),
    Lambda(lambda x: x.view(x.size(0), -1)),
)
```

#### `nn.Module`

To create your own Module, create a class a subclass `torch.nn.Module` and define a `forward` method which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors. In the `init` method, you want to call `super().__init__()` to use the resources generated by the superclass. Note that `nn.Module` objects are used as if they are functions (i.e. they are callable), but behind the scenes PyTorch  will call our `forward` method automatically.

```python
class Logistic(nn.Module):
	def __init__(self):
		super().__init__()
		self.lin = nn.Linear(784, 10)

	def forward(self, x):
		x = self.lin(x)
		return x
```

## Transformers

We can use the [Hugging Face](Hugging%20Face.md) library to work with [Transformers](../../Machine%20Learning/Deep%20Learning/Transformers.md) easily. Hugging Face comes with a `Trainer` class that allows for easy training. But you can also manually fine-tune the transformer in pure PyTorch. Assuming we have cleaned and tokenized our data, the following is the manual training loop for a single [GPU](../../Electrical%20Engineering/Digital/GPU.md):

```python
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

1. The model is defined from a pretrained checkpoint
2. The [Optimizer](../../Machine%20Learning/Deep%20Learning/Optimizers.md) is selected as Adam with additional weight decay, called `AdamW`
3. If a [GPU](../../Electrical%20Engineering/Digital/GPU.md) is detected (CUDA), then select that as the device, otherwise just use the [CPU](../../Electrical%20Engineering/Digital/CPU.md).
4. Define the number of epochs and the learning rate decay schedule. It is a linear decay over the total number of training steps, which is simply the number of epochs times the number of batches (length of the data loader)
5. Put the model into `Train` mode, this will turn "on" the dropout and batch normalization layers
6. Put the batch through the model and calculate the loss, then update the gradients of the model with `loss.backward()`, in this case the weights and biases
7. Use the `step()` function to automatically update all the model parameters with the calculated gradients. This function will also advance the learning rate schedule
8. Remember to zero out the gradients: in PyTorch, gradient accumulation is always on. If the gradient is not zeroed out, it will be summed with the next round. (Note there is also `tqdm` for a status bar, but this is not necessary)